---
layout: post
category: "List" 
title:  "A Reading List"
category: misc
date:  2022-04-19 11:15:00 -0400
modified: 2022-05-30 15:30:00 -0400
permalink: "/per_reading_list/"
description: "An unordered and highly arbitrary list of articles, books, and other things that I've read and found important enough to include here (usually this means I've taken notes on the piece), and another unordered and highly arbitrarylist containing what I would like to carefully use to learn."
tags: [personal, site]
type: "List"
status: "Ongoing"
---

<!-- header_image: /assets/2022/reading_list/race_course_at_longchamps_1970.17.114.jpg -->

---

## [Table of Contents](#top)
{:.no_toc}
* TOC
{:toc}

---

## [Ratings](#rating)
_This section exists as a place to accumulate and refine scoring metrics for content that I have read or want to read_

I make use of Leech's informal _[Book Durability][book_durc]_ scale, but extend it to all written work I read. This scale measures how much I value different written works.

>  I approximate a book's value by guessing how often I'll reread it. This rewards dense books, sure, but also ones with broad and complex messages, durability, appeal to people of different ages, and also just the pleasure and beauty of them. I think most books are not worth the time and a small number (< 8% in a highly selected sample) are worth more than one reading.
- 1/5: Not worth one reading.
- 2/5: One read maybe, if you're into the topic.
- 3/5: Worth one skim.
- 3*/5: Great fun, one read. 'Mind candy'.
- 4/5: Very good. but one read will do.
- 5?/5: Amazing. Will reread.
- 5/5: Have reread and expect to do so indefinitely. A companion.

Additionally, I rank each unread entity with 1 to 5 stars, depending on how interested I am in reading it.

[Back to top](#top)

---

## [Read Entities](#read-entities)

### [Articles](#read-articles)

- Are we living at the 'hinge of history'?
  - <https://www.bbc.com/future/article/20200923-the-hinge-of-history-long-termism-and-existential-risk>{:target="_blank"}
- Are we living at the most influential time in history?
  - <https://www.bbc.com/future/article/20200923-the-hinge-of-history-long-termism-and-existential-risk>{:target="_blank"}
- The Big Here and Long Now
  - <https://longnow.org/essays/big-here-long-now/>{:target="_blank"}
- Report likelihood ratios
  - <https://www.lesswrong.com/posts/yWCszqSCzoWTZCacN/report-likelihood-ratios>{:target="_blank"}
- Reasoning Transparency
  - <https://www.openphilanthropy.org/reasoning-transparency?>

- OpenPhilanthropy
  - Reasoning Transparency (<https://www.openphilanthropy.org/reasoning-transparency#Motivation>)

- EAF 
  - On being ambitious and celebrating failures [reviewed]

- General  
  - Things you're allowed to do (<https://milan.cvitkovic.net/writing/things_youre_allowed_to_do/>)

[Back to top](#top)

### [Books/Chapters](#read-books)

- Metamorphosis of Prime Intellect [unreviewed]
- Map and Territory [unreviewed]
- How to Actually Change Your Mind [unreviewed]
- The Elephant in the Brain [unreviewed]
- X-Risk [unreviewed]
- The Precipice [unreviewed]

[Back to top](#top)

---

## [Unread Entities](#unread-entities)

### [Articles](#unread-articles)

- OpenPhilanthropy
  - 2017 Report on Consciousness and Moral Patienthood (<https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#footnoteref119_c7p8bck>)
  - Efforts to Improve the Accuracy of Our Judgments and Forecasts (<https://www.openphilanthropy.org/blog/efforts-improve-accuracy-our-judgments-and-forecasts#Calibration_training>)

- EAF 
  - Types of information hazards: <https://forum.effectivealtruism.org/posts/ztEvQB3qw2Z4jMiXw/types-of-information-hazards>
  - Geoengineering to reduce global catastrophic risk: <https://forum.effectivealtruism.org/posts/gDGBrfkyqcxusDLzT/geoengineering-to-reduce-global-catastrophic-risk>
  - What are the different types of longtermism: <https://forum.effectivealtruism.org/posts/oCbR4oMr2waBQhXRC/what-are-the-different-types-of-longtermisms-ea-librarian>
  - What do we want the world to look like in 10 years: <https://forum.effectivealtruism.org/posts/En5ivmd2ZGDQDkDcj/what-do-we-want-the-world-to-look-like-in-10-years>
  - Issues with Futarchy: <https://forum.effectivealtruism.org/posts/E4QnGsXLEEcNysADT/issues-with-futarchy>

- Other Research
  - The Nooscope Manifested

- Concrete Problems in AI Safety
- Alignment for Advanced Machine Learning Systems
- Additive Growth
- Information Hazards in Biotechnology
- Replicating and extending the grabby aliens model
- Identifying and Assessing the Drivers of Global Catastrophic Risk: A Review and Proposal for the Global Challenges Foundation
- Embryo selection with artificial intelligence: how to evaluate and compare methods?
- Global Catastrophic Risks Survey
- Future Proof
- The AI Alignment Problem: Why It’s Hard, and Where to Start
- Artificial intelligence, systemic risks, and sustainability
- Value of GCR Information: Cost Effectiveness-Based Approach for Global Catastrophic Risk (GCR) Reduction
- Apocalypse Now? Reviving the Doomsday Argument
- The weight of suffering
- No evidence for general intelligence in a fish
- Forecasting tournaments, epistemic humility and attitude depolarization
- An Analysis and Evaluation of Methods Currently Used to Quantify the Likelihood of Existential Hazards
- Improving Judgments of Existential Risk: Better Forecasts, Questions, Explanations, Policies
- The Definition of Effective Altruism
- Technological Progress, Artificial Intelligence, and Inclusive Growth
- The transformative potential of artificial intelligence
- When Is a Crowd Wise?
- AI Safety Gridworlds
- Compute Trends Across Three Eras of Machine Learning
- Prediction: The long and short of it
- Global Solutions vs. Local Solutions for the AI Safety Problem
- Existential Risk Prevention as Global Priority
- Cheating Death in Damascus
- Existential risk and growth
- Economic growth under transformative AI
- The scope of longtermism
- Do We Really Need Deep Learning Models for Time Series Forecasting?
- Superintelligence Cannot be Contained: Lessons from Computability Theory
- AI Alignment Research Overview
- Who Should We Fear More: Biohackers, Disgruntled Postdocs, or Bad Governments? A Simple Risk Chain Model of Biorisk
- Information Hazards: A Typology of Potential Harms from Knowledge
- Likelihood functions, p-values, and the replication crisis
- Rescuing the utility function
- Report likelihoods, not p-values
- Eliciting latent knowledge: How to tell if your eyes deceive you
- Progress, Stagnation, & Collapse
- We need a Butlerian Jihad against AI
- How to prevent the coming inhuman future
- A longtermist critique of “The expected value of extinction risk reduction is positive”

[Back to top](#top)

### [Books/Chapters](#unread-books)

- Global Catastrophic Risks
- Superintelligence
- Machine Learning Refined ed. 2
- Statistical Rethinking
- Bayes Rules!
- Elements of Mathematics
- The Sequences
- Deep Learning Architectures
- Age of Em
- Statistical Consequences of Fat Tails
- LessWrong: Engines of Cognition
- LessWrong: A Map That Reflects the Territory
- Inadequate Equilibria
- Human Compatible
- All of Statistics
- Algorithms for Decision Making
- Probabilistic Machine Learning

[Back to top](#top)


[Back to top](#top)

---

## [Notes](#notes)

### [Cover Image](#cover-image)

This [cover image][cover_image]{:target="_blank"} is a French 19th Century oil on canvas artwork entitled _Race Course at Longchamps_, c. 1870. The image can be found on the [National Gallery of Art][gallery]{:target="_blank"}'s website, and is in the public domain:
> __Open Access Policy for Images of Works of Art Presumed in the Public Domain__
>
With the launch of NGA Images, the National Gallery of Art implemented an open access policy for digital images of works of art that the Gallery believes to be in the public domain. Images of these works are available free of charge for any use, commercial or non-commercial, under Creative Commons Zero (CC0). Users do not need to contact the National Gallery for authorization to use these images. They are available for download on nga.gov object pages. See Policy Details below for specific instructions and notes for users.

[cover_image]: https://www.nga.gov/collection/art-object-page.52258.html "https://www.nga.gov/collection/art-object-page.52258.html"

[gallery]: https://www.nga.gov/collection-search-result.html?sortOrder=DEFAULT&artobj_downloadable=Image_download_available&pageNumber=1&lastFacet=artobj_downloadable "https://www.nga.gov/collection-search-result.html?sortOrder=DEFAULT&artobj_downloadable=Image_download_available&pageNumber=1&lastFacet=artobj_downloadable"

[book_durc]: https://www.gleech.org/metrics/ "https://www.gleech.org/metrics/"
